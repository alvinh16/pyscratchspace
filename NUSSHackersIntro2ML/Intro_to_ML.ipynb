{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Pq36J51GcRO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71QnvUwoEuHI"
   },
   "outputs": [],
   "source": [
    "# fix the seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPAnoRUdDmU"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywBDo9uidFAv"
   },
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJ60gyB__O4H"
   },
   "source": [
    "In linear regression, the goal is to minimise the error between the given data points and the best fit line. The error is given by the vertical distance between the actual point and the regression line \\\\\n",
    "\n",
    "More formally, let us define our given data points as $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$. For each $x_i$, we will predict a value called $y_{i, pred}$. Then, the error, $ɛ_i$, at each point is defined as $$ ɛ_i = y_{i, pred} - y_i $$\n",
    "\n",
    "Now, we'd like to sum up all the errors and minimise the sum. However, we can't just add up the errors directly as the +ve and -ve errors will cancel each other out, thus under-representing the true error. \\\\\n",
    "\n",
    "Instead, we will add the squared error. We shall define the mean of the squared errors to be the **Loss Function**, $L$, i.e.\n",
    "$$ L = \\frac{1}{2n} \\sum_{i=1}^n ɛ_i^2 = \\frac{1}{2n} \\sum_{i=1}^n (y_{i, pred} - y_i)^2 \\ \\ \\ (*) $$\n",
    "\n",
    "Now, notice that in the 2-dimensional case, our best fit line is just $y = mx + c$, where $m, c$ are values that we have to figure out ($m$ and $c$ are randomly initialised). That means, for each $x_i$, we have $y_{i, pred} = mx_i + c$. That allows us to re-write the expression (*) as\n",
    "$$ L(m, c) = \\frac{1}{2n} \\sum_{i=1}^n (mx_i + c - y_i)^2 $$\n",
    "\n",
    "Note that from this substitution, we can explicitly write $L$ as a function of $m$ and $c$. This is important, as this implies that to minimise the value of the Loss Function, we just need to find optimal values for $m$ and $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQUhTKDprkpJ"
   },
   "source": [
    "## Gradient Descent Computation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdh7w2ukb9-E"
   },
   "source": [
    "It is well-known that the gradient vector, $\\nabla L(m ,c)$ gives the direction of steepest ascent for each point $m$ and $c$. Intuitively, what that means that the direction that the gradient vector, $\\nabla L(m, c)$, points in is the direction that results in the greatest increase to the value of $L(m, c)$. \\\\\n",
    "\n",
    "In the language of partial derivatives, we have\n",
    "$$\n",
    "\\nabla L(m, c) = \\begin{bmatrix}\n",
    "                \\frac{\\partial L}{\\partial m} \\\\\n",
    "                \\frac{\\partial L}{\\partial c}\n",
    "                \\end{bmatrix}\n",
    "$$\n",
    "<br>\n",
    "Computing the partial derivatives with respect to $m$, for $\\frac{\\partial L}{\\partial m}$ gives us\n",
    "$$ \\frac{\\partial L}{\\partial m} = \\frac{1}{n}\\sum_{i=1}^n x_i(mx_i + c - y_i) $$\n",
    "\n",
    "Similarly, for $\\frac{\\partial L}{\\partial c}$, we have\n",
    "$$ \\frac{\\partial L}{\\partial c} = \\frac{1}{n}\\sum_{i=1}^n (mx_i + c - y_i) $$\n",
    "\n",
    "Then, at each iteration, we can update the values of $m$ and $c$ in the following manner\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{new} \\\\\n",
    "    c_{new}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    m_{old} \\\\\n",
    "    c_{old}\n",
    "\\end{bmatrix} -k \\nabla L(m_{old}, c_{old})\n",
    "$$\n",
    "<br>\n",
    "where $k$ is the learning rate, usually set as $0.001$. \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03BgA8LRkFPn"
   },
   "source": [
    "## Implementation (Optional)\n",
    "\n",
    "In practise, generally, you are not required to implement such algorithms from scratch. Pre-built libraries with optimised implementations are easily accessible (see later section). However, a basic implementation is given here to illustrate the step by step process of the linear regression process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7T4i5eaFWSJ"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModule:\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: list[tuple[float]],\n",
    "                 lr: float = 1e-3,\n",
    "                 max_norm: float = 5.0,\n",
    "                 verbose: int = 0):\n",
    "        \"\"\"\n",
    "        data: A list of tuples with a 2 dimensions\n",
    "                E.g.: [(1, 2), (2, 3), (3, 0), ...]\n",
    "        lr: The learning rate that will be used to scale the gradient\n",
    "        max_norm: The maximum magnitude of the gradient (prevent gradient explosion)\n",
    "        verbose:\n",
    "            0 - no plots shown\n",
    "            1 - plots shown at each iteration\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "        self.m = random.random()\n",
    "        self.c = random.random()\n",
    "        self.lr = lr\n",
    "        self.max_norm = max_norm\n",
    "        self.verbose = verbose\n",
    "        self.iter_cnt = 0\n",
    "\n",
    "    def plot(self, ax):\n",
    "        \"\"\"\n",
    "        This method plots the intermediate step of the linear regression process\n",
    "        ax: An axis of a subplot\n",
    "        \"\"\"\n",
    "        data_x = [coord[0] for coord in self.data]\n",
    "        data_y = [coord[1] for coord in self.data]\n",
    "        ax.scatter(data_x, data_y, color='b')\n",
    "        best_fit_line_x_values = np.arange(start=min(data_x),\n",
    "                                           stop=max(data_x),\n",
    "                                           step=0.1)\n",
    "        best_fit_line_y_values = self.m * best_fit_line_x_values + self.c\n",
    "        squared_err = self.loss()\n",
    "        ax.plot(best_fit_line_x_values, best_fit_line_y_values, color='r')\n",
    "        ax.set_title(f\"Iteration: {self.iter_cnt}   m:{self.m:.2f}   c:{self.c:2f}   loss: {squared_err:.2f}\")\n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\"\n",
    "        Returns the value of the current loss\n",
    "        \"\"\"\n",
    "        return self.loss()\n",
    "\n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss, which is the squared error\n",
    "        \"\"\"\n",
    "        x_data = np.array([coord[0] for coord in self.data])\n",
    "        y_data = np.array([coord[1] for coord in self.data])\n",
    "        y_pred = self.m * x_data + self.c\n",
    "        err = y_pred - y_data\n",
    "        err = err * err\n",
    "        return sum(err) / (2 * self.n)\n",
    "\n",
    "    def gradient(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient\n",
    "        \"\"\"\n",
    "        grad_m = 0\n",
    "        grad_c = 0\n",
    "        for idx in range(self.n):\n",
    "            x = self.data[idx][0]\n",
    "            y = self.data[idx][1]\n",
    "            grad_m += x * (self.m * x + self.c - y) / self.n\n",
    "            grad_c += (self.m * x + self.c - y) / self.n\n",
    "        return grad_m, grad_c\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update the values of m and c according to the gradient of\n",
    "        steepest descent\n",
    "        \"\"\"\n",
    "        grad_m, grad_c = self.gradient()\n",
    "        grad_norm = (grad_m ** 2 + grad_c ** 2) ** 0.5\n",
    "        if grad_norm > self.max_norm:\n",
    "            grad_m *= self.max_norm / grad_norm\n",
    "            grad_c *= self.max_norm / grad_norm\n",
    "        prev_m = self.m\n",
    "        prev_c = self.c\n",
    "        self.m = self.m - self.lr * grad_m\n",
    "        self.c = self.c - self.lr * grad_c\n",
    "\n",
    "    def fit(self, n_iter: int = 10):\n",
    "        \"\"\"\n",
    "        Train (fit) the linear regression model\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            n_iter = max(4, n_iter)\n",
    "            fig, ax = plt.subplots(n_iter // 2, 2, figsize=(20, 5 // 2 * n_iter))\n",
    "        for i in range(n_iter):\n",
    "            self.iter_cnt += 1\n",
    "            self.step()\n",
    "            if self.verbose:\n",
    "                self.plot(ax[i // 2, i % 2])\n",
    "        if self.verbose:\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, x: float):\n",
    "        \"\"\"\n",
    "        Predicts the value using the best fit line and plot the prediction\n",
    "        if verbose is true.\n",
    "        \"\"\"\n",
    "        y_pred = self.m * x + self.c\n",
    "        if not self.verbose:\n",
    "            return y_pred\n",
    "        data_x = [coord[0] for coord in self.data]\n",
    "        data_y = [coord[1] for coord in self.data]\n",
    "        plt.scatter(data_x, data_y, color='b')\n",
    "        best_fit_line_x_values = np.arange(start=min(data_x),\n",
    "                                           stop=max(data_x),\n",
    "                                           step=0.1)\n",
    "        best_fit_line_y_values = self.m * best_fit_line_x_values + self.c\n",
    "        squared_err = self.loss()\n",
    "        plt.plot(best_fit_line_x_values, best_fit_line_y_values, color='r')\n",
    "        plt.title(f\"Prediction: y={self.m:.2f}x+{self.c:.2f}\")\n",
    "        plt.scatter([x], [y_pred], marker='X', edgecolors='black', color='green')\n",
    "        plt.show()\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu7BuNplhXxZ"
   },
   "source": [
    "## Example\n",
    "\n",
    "Run the cells to view the step-by-step process of how gradient descent is applied in a linear regression context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZivRs-TuH9Gx"
   },
   "outputs": [],
   "source": [
    "data = [(0, 1), (1, 1), (2, 3), (3, 5), (4, 5)]\n",
    "linear_regression = LinearRegressionModule(data, lr=1e-2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGv8AWeqM17o"
   },
   "outputs": [],
   "source": [
    "# initial guess\n",
    "x = 3.5\n",
    "y_pred = linear_regression.predict(x)\n",
    "print(f\"x = {x}, y_pred: {y_pred:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HDRO_ewGFG8"
   },
   "outputs": [],
   "source": [
    "# linear_regression.verbose = True\n",
    "linear_regression.fit(n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUoAC6WkGi7h"
   },
   "outputs": [],
   "source": [
    "x = 3.5\n",
    "y_pred = linear_regression.predict(x)\n",
    "print(f\"x = {x}, y_pred: {y_pred:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qLEscqGc2lU"
   },
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MndiukIyc_bu"
   },
   "source": [
    "## Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxeSIUnU_RWw"
   },
   "source": [
    "In K-Means clustering, the goal is to find optimal position for a fixed number of centroids (center points) and cluster each point based on the centroid closest to the point. In this problem, we will assume that the number of centroids, $N$, is given. In practise, the number of centroids is usually determined by some domain knowledge of the problem or some other strategy that is beyond the scope of this workshop.\n",
    "\n",
    "More formally, let us consider the case in 2-dimensions and define the set of given points as $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$. For convenience, we will also define $\\textbf{x}_i = (x_i, y_i)$.\n",
    "\n",
    "We shall also randomly initialise $N$ centroids, $(p_1, q_1), (p_2, q_2), \\dots, (p_N, q_N)$. Similarly, we will define $\\textbf{p}_i = (p_i, q_i)$.\n",
    "\n",
    "Now, an intuitive way to measure how good our clustering mechanism is to calculate the distance between each point and its corresponding centroid, then take the sum of the distances.\n",
    "\n",
    "More formally, we shall define the **Loss Function**, $L$, as the sum of all the distances. We thus have\n",
    "$$L = \\frac{1}{2n} \\sum_{i=1}^n \\min_{j} D(\\textbf{x}_i, \\textbf{p}_j)^2$$\n",
    "where the $\\displaystyle{\\min_{j} D(\\textbf{x}_i, \\textbf{p}_j)}$ denotes the distance between the point $\\textbf{x}_i$ and the centroid closest to it. The distance function, $D$, in 2-dimensions is just given by the equation\n",
    "$$ D(\\textbf{x}_i, \\textbf{p}_j) = \\sqrt{(x_i - p_j)^2 + (y_i - q_j)^2} $$\n",
    "\n",
    "Notice that since $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$ are given, the only variables are are $(p_1, q_1), (p_2, q_2), \\dots, (p_N, q_N)$. Siimlar to the linear regression problem, we just have to optimise these values to find the smallest possible value of $L(p_1, q_1, p_2, q_2, \\dots, p_N, q_N)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9S41RXSncObh"
   },
   "source": [
    "## Gradient Descent Computation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr2XS5NkxWJd"
   },
   "source": [
    "Using the language of partial derivatives, we have\n",
    "$$ \\nabla L(p_1, q_1, p_2, q_2, \\dots, p_N, q_N) = \\left( \\frac{\\partial L}{\\partial p_1}, \\frac{\\partial L}{\\partial q_1},  \\frac{\\partial L}{\\partial p_2}, \\frac{\\partial L}{\\partial q_2}, \\dots, \\frac{\\partial L}{\\partial p_N}, \\frac{\\partial L}{\\partial q_N}\\right)^T  $$\n",
    "\n",
    "While this may look slightly intimidating, the computation of each partial derivative is simply\n",
    "$$ \\frac{\\partial L}{\\partial p_j} = \\sum_{i=1}^n (p_j - x_i) \\ \\ \\ and \\ \\ \\ \\frac{\\partial L}{\\partial q_j} = \\sum_{i=1}^n (q_j - y_i)$$\n",
    "\n",
    "However, as we intend to update the centroid positions and and cluster membership, we will thus update the coordinates of the centroid corresponding to each point at every step, instead of accumulating the gradient for every point.\n",
    "\n",
    "The means, for each point $(x_i, y_i)$, we have the closest centroid $(p_j, q_j)$ to $(x_i, y_i)$. Then, we can update $(p_j, q_j)$ in the following manner:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p_{j, new} &= p_{j, old} - \\frac{1}{n_j}(p_{j, old} - x_i) \\\\\n",
    "q_{j, new} &= q_{j, old} - \\frac{1}{n_j}(q_{j, old} - y_i)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGwFmgHJkmgD"
   },
   "source": [
    "## Implementation (K-Means Clustering)\n",
    "\n",
    "In practise, generally, you are not required to implement such algorithms from scratch. Pre-built libraries with optimised implementations are easily accessible (see later section). However, a basic implementation is given here to illustrate the step by step process of the clustering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVf5PePRJ9pX"
   },
   "outputs": [],
   "source": [
    "class KMeansClusteringModule:\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: list[tuple[float]],\n",
    "                 n_centroids: int = 3,\n",
    "                 max_norm: float = 5.0,\n",
    "                 verbose: int = 0):\n",
    "        \"\"\"\n",
    "        data: A list of tuples with a fixed number of dimensions\n",
    "                E.g.: [(1, 2, 3), (1, 2, 1), (1, 2, 2), ...]\n",
    "        n_centroids: The number of centroids (centres) that we want to have for\n",
    "                        the clustering process (i.e. the number of distinct clusters)\n",
    "        max_norm: The maximum magnitude of the gradient (prevent gradient explosion)\n",
    "        verbose:\n",
    "            0 - no plots shown\n",
    "            1 - plots shown at each iteration\n",
    "            2 - plots shown at each sub-iteration\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "        self.n_dim = len(data[0])\n",
    "        self.n_centroids = n_centroids\n",
    "        self.max_norm = max_norm\n",
    "        self.verbose = verbose\n",
    "        self.iter_cnt = 0\n",
    "        self.centroids = []\n",
    "        self.clusters = {}\n",
    "        self.init_centroids()\n",
    "        self.update_clusters()\n",
    "        # Add more colours if required\n",
    "        self.COLOUR_MAPPING = {\n",
    "            0: (1, 0, 0), 1: (0, 1, 0), 2: (0, 0, 1),\n",
    "            3: (1, 0, 1), 4: (1, 1, 0), 5: (0, 1, 1),\n",
    "            6: (1, 0, 0.5), 7: (1, 0.5, 0), 8: (0, 0.5, 1),\n",
    "        }\n",
    "        # Maximum number of plots to show for each sub-iteration\n",
    "        self.num_to_show = 10\n",
    "        # Disable plots if dimensions != 2\n",
    "        if len(self.data[0]) != 2:\n",
    "            self.verbose = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def sq_distance(p1: tuple[float], p2: tuple[float]):\n",
    "        \"\"\"\n",
    "        Compute the squared distance (Euclidean distance) between two points\n",
    "        \"\"\"\n",
    "        assert len(p1) == len(p2), \\\n",
    "            \"Points should have the same number of dimensions!\"\n",
    "        n_dim = len(p1)\n",
    "        sq_dist = 0\n",
    "        for i in range(n_dim):\n",
    "            sq_dist += (p1[i] - p2[i]) ** 2\n",
    "        return sq_dist\n",
    "\n",
    "    def init_centroids(self):\n",
    "        \"\"\"\n",
    "        Randomly initialise the centroids\n",
    "        \"\"\"\n",
    "        for i in range(self.n_centroids):\n",
    "            self.centroids.append(tuple([random.random() for _ in range(self.n_dim)]))\n",
    "\n",
    "    def update_clusters(self):\n",
    "        \"\"\"\n",
    "        Update the membership of each point\n",
    "        \"\"\"\n",
    "        self.clusters = {}\n",
    "        for i in range(self.n_centroids):\n",
    "            self.clusters[i] = []\n",
    "        for curr_point in self.data:\n",
    "            min_dist = 1e99\n",
    "            closest_centroid_idx = 0\n",
    "            for i in range(self.n_centroids):\n",
    "                centroid = self.centroids[i]\n",
    "                curr_dist = KMeansClusteringModule.sq_distance(curr_point, centroid)\n",
    "                if curr_dist < min_dist:\n",
    "                    min_dist = curr_dist\n",
    "                    closest_centroid_idx = i\n",
    "            self.clusters[closest_centroid_idx].append(curr_point)\n",
    "\n",
    "    def plot(self, ax, sub_iter: int = None):\n",
    "        \"\"\"\n",
    "        Plot the intermediate step of the K-Means Clustering process\n",
    "        ax: An axis of a subplot\n",
    "        sub_iter: The current sub-iteration of the process\n",
    "                    If NONE, then do not plot sub-iteration\n",
    "        \"\"\"\n",
    "        for i in range(self.n_centroids):\n",
    "            centroid = self.centroids[i]\n",
    "            points = self.clusters[i]\n",
    "            data_x = [point[0] for point in points]\n",
    "            data_y = [point[1] for point in points]\n",
    "            ax.scatter(data_x, data_y, color=self.COLOUR_MAPPING[i])\n",
    "            ax.scatter([centroid[0]], [centroid[1]],\n",
    "                       color=self.COLOUR_MAPPING[i], marker='X', edgecolors='black')\n",
    "\n",
    "        squared_err = self.loss()\n",
    "        if sub_iter is not None:\n",
    "            ax.set_title(f\"Iteration: {self.iter_cnt}-{sub_iter}   loss: {squared_err:.2f}\")\n",
    "        else:\n",
    "            ax.set_title(f\"Iteration: {self.iter_cnt}   loss: {squared_err:.2f}\")\n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\"\n",
    "        Return the loss value\n",
    "        \"\"\"\n",
    "        return self.loss()\n",
    "\n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss, which is the squared error in this case\n",
    "        \"\"\"\n",
    "        sq_err = 0\n",
    "        for i in range(self.n_centroids):\n",
    "            centroid = self.centroids[i]\n",
    "            points = self.clusters[i]\n",
    "            for curr_point in points:\n",
    "                sq_err += KMeansClusteringModule.sq_distance(curr_point, centroid)\n",
    "        sq_err /= (2 * self.n)\n",
    "        return sq_err\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update the coordinates of the centroids based on the gradient\n",
    "        \"\"\"\n",
    "        cluster_sizes = [0] * self.n_centroids\n",
    "        if self.verbose == 2:\n",
    "            n = len(self.data)\n",
    "            fig, ax = plt.subplots(self.num_to_show // 2, 2,\n",
    "                                   figsize=(20, 5 // 2 * self.num_to_show))\n",
    "            # show first five and last five steps\n",
    "            idxs_to_show = set([0, 1, 2, 3, 4, n - 5, n - 4, n - 3, n - 2, n - 1])\n",
    "            ax_idx = 0\n",
    "        for idx, curr_point in enumerate(self.data):\n",
    "            if self.verbose == 2 and idx in idxs_to_show:\n",
    "                self.plot(ax[ax_idx // 2, ax_idx % 2], sub_iter=idx)\n",
    "                ax_idx += 1\n",
    "            min_dist = 1e99\n",
    "            closest_centroid_idx = 0\n",
    "            for i in range(self.n_centroids):\n",
    "                centroid = self.centroids[i]\n",
    "                curr_dist = KMeansClusteringModule.sq_distance(curr_point, centroid)\n",
    "                if curr_dist < min_dist:\n",
    "                    min_dist = curr_dist\n",
    "                    closest_centroid_idx = i\n",
    "            cluster_sizes[closest_centroid_idx] += 1\n",
    "            curr_centroid = self.centroids[closest_centroid_idx]\n",
    "            new_centroid = []\n",
    "            grad = []\n",
    "            lr = 1 / cluster_sizes[closest_centroid_idx]\n",
    "            # Computing the gradient\n",
    "            for i in range(len(curr_centroid)):\n",
    "                grad.append(curr_centroid[i] - curr_point[i])\n",
    "            grad = np.array(grad)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            if grad_norm > self.max_norm:\n",
    "                grad_norm *= self.max_norm / grad_norm\n",
    "            new_centroid = np.array(curr_centroid) - lr * grad\n",
    "            self.centroids[closest_centroid_idx] = tuple(new_centroid)\n",
    "            self.update_clusters()\n",
    "\n",
    "\n",
    "    def fit(self, n_iter: int = 10):\n",
    "        \"\"\"\n",
    "        Train (fit) the clustering model\n",
    "        \"\"\"\n",
    "        if self.verbose == 1:\n",
    "            fig, ax = plt.subplots(n_iter // 2, 2,\n",
    "                                   figsize=(20, 5 // 2 * n_iter))\n",
    "        for i in range(n_iter):\n",
    "            if self.verbose == 1:\n",
    "                self.plot(ax[i // 2, i % 2])\n",
    "            self.iter_cnt += 1\n",
    "            self.step()\n",
    "        if self.verbose:\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, point:tuple[float]):\n",
    "        \"\"\"\n",
    "        Predicts the cluster that the point belongs to and plot the prediction\n",
    "        if verbose is true.\n",
    "        \"\"\"\n",
    "        min_dist = 1e99\n",
    "        closest_centroid_idx = 0\n",
    "        for i in range(self.n_centroids):\n",
    "            centroid = self.centroids[i]\n",
    "            curr_dist = KMeansClusteringModule.sq_distance(point, centroid)\n",
    "            if curr_dist < min_dist:\n",
    "                min_dist = curr_dist\n",
    "                closest_centroid_idx = i\n",
    "        if not self.verbose:\n",
    "            return closest_centroid_idx, self.centroids[closest_centroid_idx]\n",
    "        for i in range(self.n_centroids):\n",
    "            centroid = self.centroids[i]\n",
    "            points = self.clusters[i]\n",
    "            data_x = [point[0] for point in points]\n",
    "            data_y = [point[1] for point in points]\n",
    "            plt.scatter(data_x, data_y, color=self.COLOUR_MAPPING[i])\n",
    "            plt.scatter([centroid[0]], [centroid[1]],\n",
    "                       color=self.COLOUR_MAPPING[i], marker='X', edgecolors='black')\n",
    "\n",
    "        plt.title(f\"Prediction\")\n",
    "        plt.scatter([point[0]], [point[1]], marker='*',\n",
    "                    edgecolors='black',\n",
    "                    color=self.COLOUR_MAPPING[closest_centroid_idx])\n",
    "        return closest_centroid_idx, self.centroids[closest_centroid_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZtaC1QyjI6E"
   },
   "source": [
    "## Example 1\n",
    "\n",
    "Run the cells to view the step-by-step process of how gradient descent is applied in a K-means clustering context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xq5ZZmSgRGbU"
   },
   "outputs": [],
   "source": [
    "data = [(-1, 0), (1, 0), (0, 1), (0, -1),\n",
    "        (9, 0), (11, 0), (10, 1), (10, -1),\n",
    "        (4, 6), (6, 6), (5, 5), (5, 7)]\n",
    "# data = [(-1, 0, 0), (1, 0, 0), (0, 1, 0), (0, -1, 0),\n",
    "#         (9, 0, 0), (11, 0, 0), (10, 1, 0), (10, -1, 0),\n",
    "#         (4, 6, 0), (6, 6, 0), (5, 5, 0), (5, 7, 0)]\n",
    "k_means_clustering_1 = KMeansClusteringModule(data, n_centroids=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtAYbb0ZNBsN"
   },
   "outputs": [],
   "source": [
    "# initial guess\n",
    "point = (0.5, 0.5)\n",
    "cluster_id, centroid = k_means_clustering_1.predict(point)\n",
    "print(f\"Point {point} is closest to the centroid: {centroid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUAUDKi2R2y_"
   },
   "outputs": [],
   "source": [
    "# k_means_clustering_1.verbose = 1\n",
    "k_means_clustering_1.fit(n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F71bP8xBHUe3"
   },
   "outputs": [],
   "source": [
    "point = (0.5, 0.5)\n",
    "cluster_id, centroid = k_means_clustering_1.predict(point)\n",
    "print(f\"Point {point} is closest to the centroid: {centroid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbyuZdvwjOTD"
   },
   "source": [
    "## Example 2\n",
    "\n",
    "Here is a slightly more complicated example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_n-SMeoodwC5"
   },
   "outputs": [],
   "source": [
    "centers = [\n",
    "    [0, 0],\n",
    "    [10, 0],\n",
    "    [5, 8.6],\n",
    "    [12, 10]\n",
    "]\n",
    "n_centroids = len(centers)\n",
    "X, y = make_blobs(n_samples=100, centers=centers, n_features=2, random_state=42)\n",
    "X = X.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu75clgEeC6q"
   },
   "outputs": [],
   "source": [
    "k_means_clustering_2 = KMeansClusteringModule(X, n_centroids=n_centroids, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xur1_hxHeLUQ"
   },
   "outputs": [],
   "source": [
    "# k_means_clustering_2.verbose = 1\n",
    "k_means_clustering_2.fit(n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epp0eigijSn2"
   },
   "outputs": [],
   "source": [
    "point = (0.5, 0.5)\n",
    "cluster_id, centroid = k_means_clustering_2.predict(point)\n",
    "print(f\"Point {point} is closest to the centroid: {centroid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mSq6Zmtr0mt"
   },
   "source": [
    "# Examples of Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-jiuNg1sJRh"
   },
   "source": [
    "## Overfitting Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVlv87Y2sM5G"
   },
   "source": [
    "One instance of overfitting is when you define too many clusters in a clustering problem. \\\\\n",
    "This will lead to the formation of clusters that do not capture the true underlying pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfRVtA3Br8oG"
   },
   "outputs": [],
   "source": [
    "data = [(-1, 0), (1, 0), (0, 1), (0, -1),\n",
    "        (9, 0), (11, 0), (10, 1), (10, -1),\n",
    "        (4, 6), (6, 6), (5, 5), (5, 7)]\n",
    "\n",
    "k_means_clustering_1 = KMeansClusteringModule(data, n_centroids=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ7-hY4KsAF8"
   },
   "outputs": [],
   "source": [
    "k_means_clustering_1.fit(n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RWq5zM6sEyR"
   },
   "outputs": [],
   "source": [
    "point = (0.5, 0.5)\n",
    "cluster_id, centroid = k_means_clustering_1.predict(point)\n",
    "print(f\"Point {point} is closest to the centroid: {centroid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLQV9UhbtHH0"
   },
   "source": [
    "# Underfitting Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK_mljdZtbW6"
   },
   "source": [
    "Similarly, an instance of underfitting is when you define too few clusters in a clustering problem. \\\\\n",
    "This will lead to the formation of clusters that do not capture the true underlying pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stCcGxvgtf90"
   },
   "outputs": [],
   "source": [
    "data = [(-1, 0), (1, 0), (0, 1), (0, -1),\n",
    "        (9, 0), (11, 0), (10, 1), (10, -1),\n",
    "        (4, 6), (6, 6), (5, 5), (5, 7)]\n",
    "\n",
    "k_means_clustering_1 = KMeansClusteringModule(data, n_centroids=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kle1lpItg_T"
   },
   "outputs": [],
   "source": [
    "k_means_clustering_1.fit(n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfaDqcLgthgx"
   },
   "outputs": [],
   "source": [
    "point = (0.5, 0.5)\n",
    "cluster_id, centroid = k_means_clustering_1.predict(point)\n",
    "print(f\"Point {point} is closest to the centroid: {centroid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7zfW5wmdS9o"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbSQdU9Emf46"
   },
   "source": [
    "Large Scale K-Means Clustering with Gradient Descent: \\\\\n",
    "https://towardsdatascience.com/large-scale-k-means-clustering-with-gradient-descent-c4d6236acd7a"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ywBDo9uidFAv",
    "VQUhTKDprkpJ",
    "03BgA8LRkFPn",
    "MndiukIyc_bu",
    "9S41RXSncObh",
    "IGwFmgHJkmgD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
